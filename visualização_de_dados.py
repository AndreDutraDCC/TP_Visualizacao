# -*- coding: utf-8 -*-
"""Visualização de Dados.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J1hW55_Ol8obal9VXPz1yO7sbG3PHtn-
"""
"""
!pip install phonemizer
!apt-get install espeak
!pip install transformers
!phonemize --version
"""
import requests
from numpy import inf

from phonemizer import phonemize
from phonemizer.separator import Separator

from transformers import pipeline

import pandas as pd

def obter_palavras_wiktionary(url_api, max_calls = inf):
    palavras = set()
    continuar = True
    cont_continue = 0
    start_title = '!'
    step = 500  # Número de palavras a serem obtidas em cada consulta
    
    while continuar:
        params = {
            'action': 'query',
            'list': 'allpages',
            'aplimit': step,
            'apfilterredir': 'nonredirects',
            'format': 'json',
            'apfrom': start_title
        }

        response = requests.get(url_api, params=params)
        data = response.json()

        for page in data['query']['allpages']:
            palavra = page['title']
            palavras.add(palavra)
        
        cont_continue += 1

        if 'continue' in data and cont_continue < max_calls:
            start_title = data['continue']['apcontinue']
        else:
            continuar = False
        
        print(f"Língua: {url_api[8:10]}, Consultas realizadas: {cont_continue}, Palavras obtidas: {len(palavras)}")
    
    return sorted(list(palavras))

# Exemplo de uso
#url_api = 'https://pt.wiktionary.org/w/api.php'  # Substitua pelo URL da API do Wiktionary desejada
#palavras_pt = obter_palavras_wiktionary(url_api, 20)
#print(f"Total de palavras em português: {len(palavras_pt)}")
#print(palavras_pt[-10:])

def obter_palavras_dirtywords(lingua):
  response = requests.get('https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/{}'.format(lingua))
  palavras = str(response.content,"utf-8")
  return palavras.split('\n')

def filtrar_palavras_validas(palavras):
  palavras_filtradas = []

  for palavra in palavras:
    palavra = palavra.strip()
    if palavra != '' and palavra[0].isalpha() and palavra[-1].isalpha():
      palavras_filtradas.append(palavra)
  
  return palavras_filtradas

def obter_fonema(palavra, lingua='pt-br'):
  pho = phonemize(palavra,
    language=lingua,
    backend="espeak",
    separator=Separator(phone=None, word=' ', syllable='|'),
    strip=True,
    preserve_punctuation=True,
    njobs=4)
  
  return str(pho)

#model_path = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
#sentiment_pipeline = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)

def obter_sentimento(palavra):
  model_path = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
  sentiment_pipeline = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)
  #global sentiment_pipeline
  
  response = sentiment_pipeline(palavra)
  return response['label'], response['score']

languages = {
    'zh': ['Chinese','Sino-Tibetan'],
    'cs': ['Czech','West Slavic'],
#    'da': ['Danish','North Germanic'],
#    'nl': ['Dutch','West Germanic'],
    'en': ['English','West Germanic'],
#    'fi': ['Finnish','Uralic'],
    'fr': ['French','Romance'],
    'de': ['German','West Germanic'],
    'hi': ['Hindi','Indic'],
    'hu': ['Hungarian','Uralic'],
    'it': ['Italian','Romance'],
#    'no': ['Norwegian','North Germanic'],
    'fa': ['Persian','Iranian'],
#    'pl': ['Polish','West Slavic'],
    'pt': ['Portuguese','Romance'],
    'ru': ['Russian','East Slavic'],
    'es': ['Spanish','Romance'],
    'sv': ['Swedish','North Germanic'],
    'tr': ['Turkish','Turkik']
}

temp = dict()
temp['cs'] = languages['cs']
temp['fa'] = languages['fa']

languages = temp

full_database = []

for language, (lang_name, lang_family) in languages.items():
  palavras_dirty = obter_palavras_dirtywords(language)
  palavras_dirty = filtrar_palavras_validas(palavras_dirty)

  full_database += [[palavra, language, lang_name, lang_family, 'dirty'] for palavra in palavras_dirty]
'''
for language, (lang_name, lang_family) in languages.items():
  url_api = "https://{}.wiktionary.org/w/api.php".format(language)
  palavras_clean = obter_palavras_wiktionary(url_api,25)
  palavras_clean = filtrar_palavras_validas(palavras_clean)

  language += '-br' if language == 'pt' else ''
  
  full_database += [[palavra, language, lang_name, lang_family, 'clean'] for palavra in palavras_clean]
'''
full_database[:5]

for line in full_database:
  w, lang, _, _, _ = line

  lang += '-fr' if lang == 'fr' else '' 

  fonema = obter_fonema(w,lang)
  line.insert(1, fonema)

full_database[:5]

for line in full_database:
  palavra = line[0]
  sentimento, grau = obter_sentimento(palavra)
  line += [sentimento, grau]

full_database[:5]

full_data = pd.DataFrame(full_database, columns = ['word','phonems','lang_code','language','lang_family','usage_context'])


full_data.to_csv('word_phonem_sentiment_data.csv', index=False)

